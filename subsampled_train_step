import matplotlib.pyplot as plt
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt
from tensorflow.keras.initializers import RandomNormal


# Block 1
class NewtonOptimizedModel(Model): 

    def __init__(self):
        super(NewtonOptimizedModel, self).__init__()
        self.dense = Dense(15, activation='tanh', input_shape=(4,), kernel_initializer=RandomNormal())
        self.dense1 = Dense(100, activation='tanh', kernel_initializer=RandomNormal())
        self.output_layer = Dense(3, activation='softmax', kernel_initializer=RandomNormal())

    def call(self, inputs):
        x = self.dense(inputs)
        x = self.dense1(x)
        return self.output_layer(x)
       
    # def regularize_hessian(self, hessian, regularization_strength=1e-9):
    #     """
    #     Regularizes the Hessian matrix to ensure it is well-conditioned before inversion.

    #     Args:
    #     - hessian: The Hessian matrix to be regularized.
    #     - regularization_strength: The regularization strength to be added to the diagonal of the Hessian.

    #     Returns:
    #     - Regularized Hessian matrix.
    #     """
    #     # Add regularization to the diagonal of the Hessian
    #     regularized_hessian = hessian + tf.eye(tf.shape(hessian)[0]) * regularization_strength
    #     return regularized_hessian

    def train_step(self, var, data):
        x, y = data
        with tf.GradientTape(persistent=True) as tape:
            tape.watch(var)
            with tf.GradientTape() as inner_tape:
                inner_tape.watch(var)
                y_pred = self(x, training=True)
                loss = self.compiled_loss(y, y_pred)
            grads = inner_tape.gradient(loss, var)
            
         # with tf.GradientTape(persistent=True) as t2:
         #     with tf.GradientTape(persistent=True) as t1:
         #         y_pred = self(x, training=True)
         #         loss = self.compiled_loss(y, y_pred)
                 
        grad_flat = tf.reshape(grads, [-1])
        loop = var.shape.num_elements()

        min_subsample_size = 1
        subsample_size = max(int(loop * self.subsampling_rate), min_subsample_size)

        # Shuffle and select subsample indices
        subsample_indices = tf.random.shuffle(tf.range(loop))[:subsample_size]
        subsample_indices = tf.sort(subsample_indices, direction='ASCENDING')    

        # Gather gradients for subsampled indices
        grad_flat_subsampled = tf.gather(grad_flat, subsample_indices)
        
        # Calculate Hessian for subsampled variables
        hessian_list = []
        for i in range(subsample_size):
        #     second_derivative = tf.gradients(grad_flat_subsampled[i], var)[0]
        #     hessian_list.append(tf.reshape(second_derivative, [-1]))
        
        # hessian_flat = tf.stack(hessian_list, axis=1)
        # hessian_filtered = tf.gather(hessian_flat, subsample_indices)
            with tf.GradientTape(persistent=True) as tape:
                tape.watch(var)
                with tf.GradientTape() as inner_tape:
                    inner_tape.watch(var)
                    # Assuming `loss_fn` is defined elsewhere or passed to this method,
                    # which computes the loss given the model's output and true labels.
                #     loss = loss_fn(y_true, model(var))  # Placeholder for actual loss computation
                #grads = inner_tape.gradient(loss, var)
            
            # Compute the Jacobian of the gradients (approximating the Hessian)
            jacobian = tape.jacobian(grad_flat_subsampled, var, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            jacobian_flat = tf.reshape(jacobian, [loop, -1])
            hessian_filtered = tf.gather(jacobian_flat, subsample_indices)
            
        n_params = tf.reduce_prod(grad_flat_subsampled.shape)
        g_vec = tf.reshape(grad_flat_subsampled, [n_params, 1])
        h_mat = tf.reshape(hessian_filtered, [n_params, n_params])
        
        # Compute average Hessian for approximation
        average_hessian = tf.reduce_mean(h_mat)
        
        # Newton's method for subsampled variables
        eps = 1e-4
        eye_eps = tf.eye(n_params) * eps
        update_filtered = tf.linalg.solve(h_mat + eye_eps, g_vec)
        
        # Prepare full update for subsampled variables
        full_update_subsampled = tf.scatter_nd(tf.reshape(subsample_indices, [-1, 1]), update_filtered, [loop, 1])
        
        # Approximate update for non-sampled variables
        # Calculate difference indices (non-sampled indices)
        all_indices = tf.range(loop)
        difference_indices = tf.sets.difference(tf.expand_dims(all_indices, 0), tf.expand_dims(subsample_indices, 0))
        difference_indices = tf.reshape(difference_indices.values, [-1])
        
        # Gather gradients for non-sampled variables
        grad_flat_non_sampled = tf.gather(grad_flat, difference_indices)
        
        # Use inverse of average Hessian for approximation
        inv_average_hessian = 1 / (average_hessian + eps)
        update_non_sampled = grad_flat_non_sampled * inv_average_hessian
        
        # Ensure update_non_sampled is properly reshaped to match the required dimensions for tf.scatter_nd
        update_non_sampled_reshaped = tf.reshape(update_non_sampled, [-1, 1])  # Reshape to ensure it has a second dimension
        
        # Prepare update for non-sampled variables
        full_update_non_sampled = tf.scatter_nd(tf.reshape(difference_indices, [-1, 1]), update_non_sampled_reshaped, [loop, 1])
        
        # Combine updates
        combined_update = full_update_subsampled + full_update_non_sampled
        
        # Apply update to variable
        update_step = var - tf.reshape(combined_update, var.shape)
        # var.assign(var_update)
        
        # return var_update
        # Compute update step
        #update_step = tf.matmul(inv_hessian, tf.expand_dims(grad, -1))
        
        # Apply update
        var.assign_sub(tf.squeeze(update_step))
    
        # del t1, t2
        # self.compiled_metrics.update_state(y, y_pred)
        # return {m.name: m.result() for m in self.metrics}
        del tape, inner_tape
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}

    
      
   ######################################################
   ######################## DATA ########################
   ######################################################
   
# Load Data
file_path = '/path_to/iris.csv'
data = pd.read_csv(file_path)

# Prepare Data
X = data.iloc[:, 0:4].values
y = data.iloc[:, 4].values
encoder = LabelEncoder()
encoder.fit(y)
encoded_Y = encoder.transform(y)
dummy_y = to_categorical(encoded_Y)

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X, dummy_y, test_size=0.2, random_state=42)

class StoreWeightNormCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(StoreWeightNormCallback, self).__init__()
        self.weight_norms_per_epoch = []

    def on_epoch_end(self, epoch, logs=None):
        weights = self.model.get_weights()
        weight_norms = [np.linalg.norm(w) for w in weights]
        self.weight_norms_per_epoch.append(weight_norms)
        print(f"Epoch {epoch + 1}, Weight norms: {weight_norms}")
        
# Create and Compile Model
model = NewtonOptimizedModel()
model.compile(loss='categorical_crossentropy', metrics=['accuracy'])

# Training Parameters
batch_size = X_train.shape[0]

# Train Model
#model.fit(X_train, y_train, batch_size=batch_size, epochs=5, validation_split=0.2)

# Evaluate Model
#scores = model.evaluate(X_test, y_test, batch_size=batch_size, verbose="auto")
#print(f"Accuracy: {scores[1]*100}")

# Train the model with the custom callback

callback = StoreWeightNormCallback()

history = model.fit(X_train, y_train, batch_size=batch_size, epochs=300, 
          validation_split=0.25, callbacks=[callback])

# After training, access the stored weight norms
epoch_weight_norms = callback.weight_norms_per_epoch

# You can now print or analyze epoch_weight_norms
print(epoch_weight_norms)

# Evaluate the model
scores = model.evaluate(X_test, y_test, batch_size=batch_size, verbose="auto")
print(f"Accuracy: {scores[1]*100}")

# Plot Training and Validation Loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()




